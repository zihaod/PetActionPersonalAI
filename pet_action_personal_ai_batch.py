# -*- coding: utf-8 -*-
"""Pet_Action_Personal_AI_Batch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NrGl8e64C7dQ8lqAtvOdTQZpy7RpQFbJ

#Prep
"""

from google.colab import drive
drive.mount('/content/drive')

#import cvxpy as cp
import numpy as np
#import scipy
import math
#from scipy.special import rel_entr
#from scipy.stats import entropy
import copy
#import pickle
import matplotlib.pyplot as plt
import sklearn
import os
import glob
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch
import json

from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

"""#Data Processing"""

def read_json_data(fpath):
    with open(fpath, 'r') as f:
        data = json.load(f)
    return data

# Load data from main database
cat_data = []
for fname in os.listdir('/content/drive/MyDrive/pet_project/pet_action/6月10日json合集'):
    if fname.startswith('猫') and fname.endswith('.json'):
        fpath = '/content/drive/MyDrive/pet_project/pet_action/6月10日json合集/' + fname
        data = read_json_data(fpath)
        cat_data.append(data)

small_dog_data = []
for fname in os.listdir('/content/drive/MyDrive/pet_project/pet_action/6月10日json合集'):
    if fname.startswith('小狗') and fname.endswith('.json'):
        fpath = '/content/drive/MyDrive/pet_project/pet_action/6月10日json合集/' + fname
        data = read_json_data(fpath)
        small_dog_data.append(data)

mid_dog_data = []
for fname in os.listdir('/content/drive/MyDrive/pet_project/pet_action/6月10日json合集'):
    if fname.startswith('中型狗') and fname.endswith('.json'):
        fpath = '/content/drive/MyDrive/pet_project/pet_action/6月10日json合集/' + fname
        data = read_json_data(fpath)
        mid_dog_data.append(data)

"""#Dataset"""

def list_minus(l1, l2):
    return [l2[i] - l1[i] for i in range(len(l1))]

def list_acc_minus(l1, l2):
    return [(l2[i] - l1[i])/100 if i < 3 else l2[i]/100 for i in range(len(l1))]

def list_cat(l1, l2):
    return [l1[i] + l2[i] for i in range(len(l1))]



class ActionDataset(Dataset):
    def __init__(self, pet_data, actions, seq_len=10, step_size=1, diff_mode='all', augment=None, aug_prob=0.5):
        self.augment = augment
        self.aug_prob = aug_prob
        self.pet_data = pet_data
        self.seq_len = seq_len
        self.step_size = step_size
        self.diff_mode = diff_mode
        self.actions = actions

        self.data = []
        self.labels = []

        for file_data in pet_data:
            curr_idx = seq_len
            while curr_idx <= len(file_data):
                x = [[d['acc_x'], d['acc_y'], d['acc_z'],
                      d['gry_x'], d['gry_y'], d['gry_z']]
                      for d in file_data[curr_idx-seq_len : curr_idx]]

                y = file_data[curr_idx-1]['label']
                self.data.append(x)
                self.labels.append(self.actions.index(y))
                curr_idx += step_size

        if self.diff_mode == "all":
            for i in range(len(self.data)):
                self.data[i] = [list_minus(self.data[i][j], self.data[i][j-1])
                                for j in range(1, len(self.data[i]))]

        elif self.diff_mode == "acc":
            for i in range(len(self.data)):
                self.data[i] = [list_acc_minus(self.data[i][j], self.data[i][j-1])
                                for j in range(1, len(self.data[i]))]



    def interpolate(self, X, missing_idxs):

        def prev_valid_idx(idx):
            for i in range(idx, -1, -1):
                if not X[i] in missing_idxs:
                    return i
        def next_valid_idx(idx):
            for i in range(idx, len(X), 1):
                if not X[i] in missing_idxs:
                    return i

        fill_values = {}
        for idx in missing_idxs:
            prev_idx = prev_valid_idx(idx)
            next_idx = next_valid_idx(idx)
            fill_values[idx] = (X[prev_idx] + (X[next_idx] - X[prev_idx]) / (next_idx - prev_idx))

        return fill_values



    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        X = torch.tensor(self.data[idx])
        Y = self.labels[idx]

        if not self.augment:
            return X, Y

        else:
            p = torch.rand(1)[0]
            if p < self.aug_prob:
                aug_idxs = np.random.choice(list(range(1, len(X)-1)), 2, replace=False)

                if self.augment == 'avg':
                    aug_samples = self.interpolate(X, aug_idxs)
                    for aug_idx in aug_idxs:
                        X[aug_idx] = aug_samples[aug_idx]

            return X, Y


class ActionDatasetFromGroup(Dataset):
    def __init__(self, pet_data, actions):
        self.actions = actions

        self.data = pet_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

"""#Model"""

class AirModel(nn.Module):
    def __init__(self, num_class, input_size, hidden_size=30, num_layers=10):
        super().__init__()
        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, num_class)

    def forward(self, x):
        x, _ = self.lstm(x)
        x = x[:,-1]
        x = self.linear(x)
        return x

def export_lstm_to_bin(model_name, model, hidden_size):

    save_dir = f'./{model_name}/'
    os.makedirs(save_dir, exist_ok=True)

    for name, param in model.named_parameters():
        if param.requires_grad:
            tensor = param.data.unsqueeze(1) if param.data.dim() == 1 else param.data
            if 'lstm' in name:
                w_or_b = 'w' if 'weight' in name else 'b'
                i_or_h = 'i' if 'ih' in name else 'h'
                ifgo = {0: 'i', 1: 'f', 2: 'g', 3: 'o'}
                layer = name[-1]

                for i in range(4):
                    sub_tensor = tensor[hidden_size*i:hidden_size*(i+1)]
                    fname = w_or_b + i_or_h + ifgo[i] + layer + '.bin'
                    fname = save_dir + fname

                    tensor_bytes = sub_tensor.detach().cpu().numpy().tobytes()

                    with open(fname, 'wb') as f:
                        f.write(tensor_bytes)

            elif 'linear' in name:
                fname = 'w_proj.bin' if 'weight' in name else 'b_proj.bin'
                fname = save_dir + fname

                tensor_bytes = tensor.detach().cpu().numpy().tobytes()

                with open(fname, 'wb') as f:
                    f.write(tensor_bytes)

"""#Training and Evaluation Scripts"""

def evaluate(model, val_loader, device):
    criterion = torch.nn.CrossEntropyLoss()

    total = 0
    correct = 0
    losses = []
    predictions = []
    labels = []

    model.eval()

    print('-------Evaluation Start-------')
    for i_batch, data_batch in enumerate(val_loader):

            x_batch = data_batch[0].float().to(device)
            y_batch = data_batch[1].to(device)

            output = model(x_batch)
            loss = criterion(output, y_batch)

            preds = torch.argmax(output, dim=1)

            total += y_batch.shape[0]
            correct += torch.sum(preds==y_batch)

            losses.append(loss.item())
            predictions.append(preds.detach().cpu().numpy())
            labels.append(y_batch.detach().cpu().numpy())

    final_loss = sum(losses) / len(losses)
    acc = correct / total

    print('------------------------------')
    print(f'Val Loss: {final_loss}')
    print(f'Val Acc: {acc}')
    print('------------------------------')

    return np.concatenate(predictions), np.concatenate(labels)

def train(model, train_loader, val_loader, num_epoch, batch_size, lr, device='cuda'):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    criterion = torch.nn.CrossEntropyLoss()

    model.to(device)

    print('-------Training Start-------')
    for _epoch in range(num_epoch):
        print(f'Epoch: {_epoch}')
        losses = []
        model.train()

        for i_batch, data_batch in enumerate(train_loader):
            #print(f'  iter: {i_batch}/{len(train_loader)}')

            x_batch = data_batch[0].float().to(device)
            y_batch = data_batch[1].to(device)

            output = model(x_batch)
            loss = criterion(output, y_batch)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            losses.append(loss.item())
            #print(f'    iter_loss: {loss}')

        train_loss = sum(losses) / len(losses)
        evaluate(model, val_loader, device)

        print('Epoch: ', _epoch, ' train_loss: ', train_loss)

def get_json_files(data_dir):
    json_files = []

    # Walk through directory and all subdirectories
    for root, dirs, files in os.walk(data_dir):
        # Find all .json files in current directory
        for file in files:
            if file.lower().endswith('.json'):
                # Get full path and append to list
                full_path = os.path.join(root, file)
                json_files.append(full_path)

    return json_files

"""#Start personal AI training"""

num_epoch = 3
batch_size = 1024
val_batch_size = 512
lr = 1e-3
device = 'cuda' if torch.cuda.is_available() else 'cpu'
date = '20250109'

fpaths = []
data_dir = '/content/drive/MyDrive/pet_project/pet_action/personal_AI_test_data/20250109数据标注/'

fpaths = get_json_files(data_dir)
keys = set(["_".join(p.split("/")[-1].split("_")[1:4]) for p in fpaths])

fpaths_dict = {}
for k in keys:
    fpaths_dict[k] = []
    for p in fpaths:
        if k in p:
            fpaths_dict[k].append(p)

fpaths_dict

"""##Cat"""

# base dataset
actions = ['walk', 'sleep', 'run', 'lick', 'play', 'jump', 'feed', 'roll', 'scratch', 'rest']
dataset = ActionDataset(cat_data, actions, seq_len=20, step_size=1, diff_mode="acc", augment=None)
#train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])
#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

# Get Base data labels and split based on label
base_labels = set([dataset[i][1] for i in range(len(dataset))])
base_split_data = {}
for label in base_labels:
    base_split_data[label] = []

for d in dataset:
    label = d[1]
    base_split_data[label].append(d)

print([len(base_split_data[i]) for i in range(len(base_split_data))])

"""**Perform training for each identity**"""

import subprocess

idx2multiplier = dict()
idx2multiplier[0] = 1
idx2multiplier[1] = 1
idx2multiplier[2] = 8
idx2multiplier[3] = 1
idx2multiplier[4] = 7
idx2multiplier[5] = 10
idx2multiplier[6] = 1
idx2multiplier[7] = 15
idx2multiplier[8] = 20
idx2multiplier[9] = 1


#for fname in os.listdir(data_dir):
for k, fpaths in fpaths_dict.items():
    if '猫' not in k:
        continue
    #fpath = data_dir + fname
    #fname = fpath.split('/')[-1].split('.')[0]
    #print(fpath)
    #print(fname)
    print(k)
    personal_data = [read_json_data(fpath) for fpath in fpaths]

    # personal dataset
    personal_dataset = ActionDataset(personal_data, actions, seq_len=20, step_size=1, diff_mode="acc", augment=None)
    personal_loader = DataLoader(personal_dataset, batch_size=val_batch_size, shuffle=False)

    # Get personal data labels and split based on label
    personal_labels = set([personal_dataset[i][1] for i in range(len(personal_dataset))])
    personal_split_data = {}
    for label in personal_labels:
        personal_split_data[label] = []

    for d in personal_dataset:
        label = d[1]
        personal_split_data[label].append(d)

    # Construct combined dataset
    target_label_idxs = personal_labels
    combined_data = []

    for label in base_labels:

        if label in target_label_idxs:
            personal_len = len(personal_split_data[label])
            base_len = len(base_split_data[label])

            for _ in range(idx2multiplier[label]):

                for _ in range(max(base_len // personal_len, 1)):
                    combined_data.extend(personal_split_data[label])

                combined_data.extend(base_split_data[label])

            print(len(personal_split_data[label]))
            print(len(base_split_data[label]))

        else:
            combined_data.extend(base_split_data[label])

    combined_dataset = ActionDatasetFromGroup(combined_data, actions)
    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)


    model = torch.load('/content/drive/MyDrive/pet_project/pet_action/cat_models/cat_hidden_72_seq_20_scaledown_100_accdiff_20240622.pt').to(device)
    try:

      train(model, combined_loader, personal_loader, num_epoch, batch_size, lr, device)
      predictions, labels = evaluate(model, personal_loader, device=device)
      predictions, labels = evaluate(model, data_loader, device=device)

      model.cpu()
      #model_name = 'train_' + '_'.join(fname.split('_')[1:-1]) + '_20241218'
      model_name = 'train_' + k + '_' + date
      #torch.save(model, f'./{model_name}.pt')
      # Save model in binary format
      export_lstm_to_bin(model_name, model, 72)
      zip_cmd = ["zip", "-r", model_name + ".zip", model_name]

      subprocess.run(zip_cmd)

    except:
      print(f"Failed file {fname}")

"""##Small Dog"""

# base dataset
actions = ['toy', 'jump', 'rest', 'walk', 'sleep', 'feed', 'run', 'tail', 'roll']
dataset = ActionDataset(small_dog_data, actions, seq_len=20, step_size=1, diff_mode="acc", augment=None)
#train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])
#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

# Get Base data labels and split based on label
base_labels = set([dataset[i][1] for i in range(len(dataset))])
base_split_data = {}
for label in base_labels:
    base_split_data[label] = []

for d in dataset:
    label = d[1]
    base_split_data[label].append(d)

print([len(base_split_data[i]) for i in range(len(base_split_data))])

"""**Perform training for each identity**"""

import subprocess

idx2multiplier = dict()
idx2multiplier[0] = 2
idx2multiplier[1] = 6
idx2multiplier[2] = 1
idx2multiplier[3] = 1
idx2multiplier[4] = 4
idx2multiplier[5] = 2
idx2multiplier[6] = 2
idx2multiplier[7] = 10
idx2multiplier[8] = 8


#for fname in os.listdir(data_dir):
for k, fpaths in fpaths_dict.items():
    if '小狗' not in k:
        continue
    #fpath = data_dir + fname
    #fname = fpath.split('/')[-1].split('.')[0]
    #print(fpath)
    #print(fname)
    print(k)
    personal_data = [read_json_data(fpath) for fpath in fpaths]

    # personal dataset
    personal_dataset = ActionDataset(personal_data, actions, seq_len=20, step_size=1, diff_mode="acc", augment=None)
    personal_loader = DataLoader(personal_dataset, batch_size=val_batch_size, shuffle=False)

    # Get personal data labels and split based on label
    personal_labels = set([personal_dataset[i][1] for i in range(len(personal_dataset))])
    personal_split_data = {}
    for label in personal_labels:
        personal_split_data[label] = []

    for d in personal_dataset:
        label = d[1]
        personal_split_data[label].append(d)

    # Construct combined dataset
    target_label_idxs = personal_labels
    combined_data = []

    for label in base_labels:

        if label in target_label_idxs:
            personal_len = len(personal_split_data[label])
            base_len = len(base_split_data[label])

            for _ in range(idx2multiplier[label]):

                for _ in range(max(base_len // personal_len, 1)):
                    combined_data.extend(personal_split_data[label])

                combined_data.extend(base_split_data[label])

            print(len(personal_split_data[label]))
            print(len(base_split_data[label]))

        else:
            combined_data.extend(base_split_data[label])

    combined_dataset = ActionDatasetFromGroup(combined_data, actions)
    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)


    model = torch.load('/content/drive/MyDrive/pet_project/pet_action/dog_models/dog_small_hidden_72_seq_20_scaledown_100_accdiff_20240629.pt').to(device)

    try:

      train(model, combined_loader, personal_loader, num_epoch, batch_size, lr, device)
      predictions, labels = evaluate(model, personal_loader, device=device)
      predictions, labels = evaluate(model, data_loader, device=device)

      model.cpu()
      #model_name = 'train_' + '_'.join(fname.split('_')[1:-1]) + '_20241218'
      model_name = 'train_' + k + '_' + date
      #torch.save(model, f'./{model_name}.pt')
      # Save model in binary format
      export_lstm_to_bin(model_name, model, 72)
      zip_cmd = ["zip", "-r", model_name + ".zip", model_name]

      subprocess.run(zip_cmd)

    except:
      print(f"Failed file {fname}")

"""##Mid Dog"""

# base dataset
actions = ['toy', 'jump', 'rest', 'walk', 'sleep', 'feed', 'run', 'tail', 'roll']
dataset = ActionDataset(mid_dog_data, actions, seq_len=20, step_size=1, diff_mode="acc", augment=None)
#train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])
#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

# Get Base data labels and split based on label
base_labels = set([dataset[i][1] for i in range(len(dataset))])
base_split_data = {}
for label in base_labels:
    base_split_data[label] = []

for d in dataset:
    label = d[1]
    base_split_data[label].append(d)

print([len(base_split_data[i]) for i in range(len(base_split_data))])

"""**Perform training for each identity**"""

import subprocess

idx2multiplier = dict()
idx2multiplier[0] = 2
idx2multiplier[1] = 6
idx2multiplier[2] = 1
idx2multiplier[3] = 1
idx2multiplier[4] = 4
idx2multiplier[5] = 2
idx2multiplier[6] = 2
idx2multiplier[7] = 10
idx2multiplier[8] = 8


#for fname in os.listdir(data_dir):
for k, fpaths in fpaths_dict.items():
    if '大狗' not in k:
        continue
    #fpath = data_dir + fname
    #fname = fpath.split('/')[-1].split('.')[0]
    #print(fpath)
    #print(fname)
    print(k)
    personal_data = [read_json_data(fpath) for fpath in fpaths]

    # personal dataset
    personal_dataset = ActionDataset(personal_data, actions, seq_len=20, step_size=1, diff_mode="acc", augment=None)
    personal_loader = DataLoader(personal_dataset, batch_size=val_batch_size, shuffle=False)

    # Get personal data labels and split based on label
    personal_labels = set([personal_dataset[i][1] for i in range(len(personal_dataset))])
    personal_split_data = {}
    for label in personal_labels:
        personal_split_data[label] = []

    for d in personal_dataset:
        label = d[1]
        personal_split_data[label].append(d)

    # Construct combined dataset
    target_label_idxs = personal_labels
    combined_data = []

    for label in base_labels:

        if label in target_label_idxs:
            personal_len = len(personal_split_data[label])
            base_len = len(base_split_data[label])

            for _ in range(idx2multiplier[label]):

                for _ in range(max(base_len // personal_len, 1)):
                    combined_data.extend(personal_split_data[label])

                combined_data.extend(base_split_data[label])

            print(len(personal_split_data[label]))
            print(len(base_split_data[label]))

        else:
            combined_data.extend(base_split_data[label])

    combined_dataset = ActionDatasetFromGroup(combined_data, actions)
    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)


    model = torch.load('/content/drive/MyDrive/pet_project/pet_action/dog_models/dog_small_hidden_72_seq_20_scaledown_100_accdiff_20240629.pt').to(device)

    try:

      train(model, combined_loader, personal_loader, num_epoch, batch_size, lr, device)
      predictions, labels = evaluate(model, personal_loader, device=device)
      predictions, labels = evaluate(model, data_loader, device=device)

      model.cpu()
      #model_name = 'train_' + '_'.join(fname.split('_')[1:-1]) + '_20241218'
      model_name = 'train_' + k + '_' + date
      #torch.save(model, f'./{model_name}.pt')
      # Save model in binary format
      export_lstm_to_bin(model_name, model, 72)
      zip_cmd = ["zip", "-r", model_name + ".zip", model_name]

      subprocess.run(zip_cmd)

    except:
      print(f"Failed file {fname}")